# -*- coding: utf-8 -*-

import os
import os.path as osp
import numpy as np
import pandas as pd

from ddf_utils.str import to_concept_id, format_float_digits
from ddf_utils.factory import ihme

from functools import partial
from itertools import product
from zipfile import ZipFile

import dask
import dask.dataframe as dd

from queries import QUERIES


# global configurations
source_dir = '../source/'
output_dir = '../../'
CONF = {'split_by': 'sex',
        'dimension_order': ['sex', 'age', 'cause', 'location', 'year']}

# number formatter
formatter = partial(format_float_digits, digits=2)

# settings for read_csv
DTYPES = dict(location=np.int32, sex=np.int8, year=np.int16, cause=np.int16, age=np.int16, val=np.float,
              measure=np.int8, metric=np.int8)


class QueryProcessor:
    def __init__(self, query, metadata, config=None):
        "proecss the downloaded files by the query"
        self.query = query
        self.config = config
        self.metadata = metadata

        # adding default configs
        if self.config is None:
            self.config = {'split_by': None,
                           'dimension_order': ['sex', 'age', 'cause', 'location', 'year']}

    def load_data(self, n):
        # print(n)
        f = osp.join(source_dir, self.query['id'], n)
        zf = ZipFile(f)
        fname = osp.splitext(n)[0] + '.csv'
        data = pd.read_csv(zf.open(fname), dtype=DTYPES)
        # for now we only consider the medium value, so upper/lower bounds are dropped.
        # FIXME: add config option to allow selecting
        data = data.drop(['upper', 'lower'], axis=1)

        # double check things
        for k, v in self.query.items():
            if k in ['id', 'context', 'email']:
                continue
            assert set(data[k].unique().tolist()).issubset(set(v))

        return data

    def serve_datapoint(self, df, concept):
        cols_new = ['location', 'sex', 'age', 'cause', 'year', concept]
        df.columns = cols_new

        causes = df.cause.unique()
        sexes = df.sex.unique()
        # if concept == 'mmr_rate':
        #     print(df.sex.unique())
        for g_ in product(causes, sexes):
            print(f"working on group {g_}")
            df_ = df[(df.cause == g_[0]) & (df.sex == g_[1])]
            if df_.empty:
                continue
            # print(g_)
            # print(df_.age.unique())
            # print(len(df_.year.unique()))
            # print(len(df_.location.unique()))
            df_ = df_.rename(columns={'val': concept})
            cause = 'cause-{}'.format(g_[0])
            sex = 'sex-{}'.format(g_[1])
            by = ['location', sex, 'age', cause, 'year']
            file_name = 'ddf--datapoints--' + concept + '--by--' + '--'.join(by) + '.csv'
            file_path = osp.join(output_dir, file_name)
            df_[concept] = df_[concept].map(formatter)
            df_[cols_new].sort_values(by=['location', 'sex', 'age', 'year']).to_csv(file_path, index=False)

    def get_measures(self):
        """return all measures which will generated by the query."""
        md = self.metadata
        metric = md['metric'].set_index('id')['name'].to_dict()
        measure = md['measure'].set_index('id')['name'].to_dict()
        measure_metric_combinations = product(self.query['measure'], self.query['metric'])
        for g in measure_metric_combinations:
            name = measure[g[0]] + ' ' + metric[g[1]]
            concept = to_concept_id(name)
            yield (concept, name)

    def run(self):
        """The entire process to create all datapoints files from the query.

        1. load all data from source dir: dask dataframe
        2. select data by measure/metric pair, creating dataframe for concept
        3. rename columns
        4. split by entity, if it's set in configuration
        """
        source_dir = os.path.join('../source/', self.query['id'])
        print('loading data from {}'.format(source_dir))
        data_full = dd.from_delayed([dask.delayed(self.load_data)(f) for f in os.listdir(source_dir) if f.endswith('.zip')], meta=DTYPES)

        md = self.metadata
        metric = md['metric'].set_index('id')['name'].to_dict()
        measure = md['measure'].set_index('id')['name'].to_dict()

        split_by = self.config['split_by']
        by = self.config['dimension_order']
        cols = by.copy()
        cols.append('val')

        all_measures = list()
        measure_metric_combinations = product(self.query['measure'], self.query['metric'])
        for g in measure_metric_combinations:
            name = measure[g[0]] + ' ' + metric[g[1]]
            print(f'creating dattpoints for {name}')
            concept = to_concept_id(name)
            all_measures.append((concept, name))

            df = data_full.loc[(data_full.measure == g[0]) & (data_full.metric == g[1]), cols].compute()
            cols_rename = by.copy()
            cols_rename.append(concept)
            df.columns = cols_rename
            serve_datapoint(df, by, concept, split_by)


def split_dataframe_by(df, by):
    for g, df_g in df.groupby(by):
        if isinstance(by, list):
            yield (dict(zip(by, g)), df_g)
        else:
            yield ({by: g}, df_g)


def serve_datapoint(df, by, concept, split_by=False):
    df_ = df.copy()
    df_[concept] = df_[concept].map(formatter)
    df_ = df_.sort_values(by=by)

    if split_by:
        for group, df_g in split_dataframe_by(df_, split_by):
            by_ = []
            for b in by:
                if b in group.keys():
                    by_.append('{}-{}'.format(b, group[b]))
                else:
                    by_.append(b)
            by_str = '--'.join(by_)
            filename = 'ddf--datapoints--{}--by--{}.csv'.format(concept, by_str)
            df_g.to_csv(os.path.join(output_dir, filename), index=False)
    else:
        by_str = '--'.join(by)
        filename = 'ddf--datapoints--{}--by--{}.csv'.format(concept, by_str)
        df_.to_csv(os.path.join(output_dir, filename), index=False)


def serve_entities(md):
    sexs = md['sex'].copy()
    sexs.columns = ['sex', 'name', 'short_name']
    sexs[['sex', 'name']].to_csv('../../ddf--entities--sex.csv', index=False)

    causes = md['cause'].copy()
    causes = causes.drop(['most_detailed', 'sort_order'], axis=1)
    causes.columns = ['cause', 'label', 'name', 'medium_name', 'short_name']
    causes.to_csv('../../ddf--entities--cause.csv', index=False)

    locations = md['location'].copy()
    locations = locations[locations.location_id != 'custom'].drop(['location_id', 'enabled'], axis=1)
    locations = locations.rename(columns={'id': 'location'})
    locations = locations[['location', 'type', 'name', 'medium_name', 'short_name']]
    locations.location = locations.location.map(lambda x: str(int(x)))
    locations.to_csv('../../ddf--entities--location.csv', index=False)

    ages = md['age'].copy()
    ages = ages.sort_values(by='sort')[['id', 'name', 'short_name', 'type']]
    ages.columns = ['age', 'name', 'short_name', 'type']
    ages.to_csv('../../ddf--entities--age.csv', index=False)

    rei = md['rei'].copy()
    rei = rei.drop(['most_detailed', 'enabled'], axis=1)
    rei.columns = ['rei', 'type', 'name', 'short_name']
    rei.to_csv('../../ddf--entities--rei.csv', index=False)


def main():
    md = ihme.IHMELoader().load_metadata()

    all_measures = list()
    for q in QUERIES:
        p = QueryProcessor(q, md, config=CONF)
        for concept, name in p.get_measures():
            all_measures.append([concept, name])
        p.run()

    # entities
    serve_entities(md)

    # concepts
    cont_cdf = pd.DataFrame(all_measures, columns=['concept', 'name'])
    cont_cdf['concept_type'] = 'measure'
    cont_cdf.to_csv('../../ddf--concepts--continuous.csv', index=False)

    dis_cdf = pd.DataFrame([
        ['name', 'Name', 'string'],
        ['short_name', 'Short Name', 'string'],
        ['medium_name', 'Medium Name', 'string'],
        ['long_name', 'Long Name', 'string'],
        ['location', 'Location', 'entity_domain'],
        ['sex', 'Sex', 'entity_domain'],
        ['age', 'Age', 'entity_domain'],
        ['cause', 'Cause', 'entity_domain'],
        ['rei', 'Risk/Etiology/Impairment', 'entity_domain'],
        ['label', 'Label', 'string'],
        ['year', 'Year', 'time'],
        ['type', 'Type', 'string']
    ], columns=['concept', 'name', 'concept_type'])

    dis_cdf.sort_values(by='concept').to_csv('../../ddf--concepts--discrete.csv', index=False)

    print("Done.")


if __name__ == '__main__':
    main()
